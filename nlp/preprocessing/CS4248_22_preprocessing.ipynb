{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7HJ5aVHrRWN",
    "outputId": "877bf079-e951-41e8-ae26-02449385c2b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48854\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Uncomment this if these packages are not downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Increase the maximum field size limit\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "trainData = []\n",
    "with open('fulltrain.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        trainData.append(row)\n",
    "\n",
    "df = pd.DataFrame(trainData, columns=['Label', 'Text'])\n",
    "\n",
    "\n",
    "testData = []\n",
    "with open('balancedtest.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        testData.append(row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df2 = pd.DataFrame(testData, columns=['Label', 'Text'])\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Label                                               Text  \\\n",
      "0         1  A little less than a decade ago, hockey fans w...   \n",
      "1         1  The writers of the HBO series The Sopranos too...   \n",
      "2         1  Despite claims from the TV news outlet to offe...   \n",
      "3         1  After receiving 'subpar' service and experienc...   \n",
      "4         1  After watching his beloved Seattle Mariners pr...   \n",
      "...     ...                                                ...   \n",
      "48849     4  The ruling Kuomintang (KMT) has claimed owners...   \n",
      "48850     4  The Taipei city government has encouraged the ...   \n",
      "48851     4  President Ma Ying-jeou said Friday that a park...   \n",
      "48852     4  The families of the four people who were kille...   \n",
      "48853     4  The Ministry of Finance will make public on Sa...   \n",
      "\n",
      "                                      tokenized_sentence  \\\n",
      "0      [A little less than a decade ago, hockey fans ...   \n",
      "1      [The writers of the HBO series The Sopranos to...   \n",
      "2      [Despite claims from the TV news outlet to off...   \n",
      "3      [After receiving 'subpar' service and experien...   \n",
      "4      [After watching his beloved Seattle Mariners p...   \n",
      "...                                                  ...   \n",
      "48849  [The ruling Kuomintang (KMT) has claimed owner...   \n",
      "48850  [The Taipei city government has encouraged the...   \n",
      "48851  [President Ma Ying-jeou said Friday that a par...   \n",
      "48852  [The families of the four people who were kill...   \n",
      "48853  [The Ministry of Finance will make public on S...   \n",
      "\n",
      "                                              way11_text  \n",
      "0      [A little less than a decade ago, hockey fans ...  \n",
      "1      [The writers of the HBO series The Sopranos to...  \n",
      "2      [Despite claims from the TV news outlet to off...  \n",
      "3      [After receiving 'subpar' service and experien...  \n",
      "4      [After watching his beloved Seattle Mariners p...  \n",
      "...                                                  ...  \n",
      "48849  [The ruling Kuomintang (KMT) has claimed owner...  \n",
      "48850  [The Taipei city government has encouraged the...  \n",
      "48851  [President Ma Ying-jeou said Friday that a par...  \n",
      "48852  [The families of the four people who were kill...  \n",
      "48853  [The Ministry of Finance will make public on S...  \n",
      "\n",
      "[48854 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'way11_train.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    your_object = pickle.load(file)\n",
    "\n",
    "print(your_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase + remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# way6 \n",
    "def preprocess_text(text):\n",
    "    \n",
    "    tokenized_sentences = sent_tokenize(text)\n",
    "    new_text = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        tokenized_words = word_tokenize(sentence)\n",
    "        new_sentence = []\n",
    "        for word in tokenized_words:\n",
    "            lower_word = word.lower()\n",
    "            if lower_word not in stop_words:\n",
    "                new_sentence.append(lower_word)\n",
    "        new_text.append(new_sentence)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = df.copy()\n",
    "dfl['tokenized_sentence'] = df['Text'].apply(sent_tokenize)\n",
    "dfl['way6_text'] = df['Text'].apply(preprocess_text)\n",
    "dfl.to_pickle('way6_train.pkl')\n",
    "\n",
    "dfl2 = df2.copy()\n",
    "dfl2['tokenized_sentence'] = df2['Text'].apply(sent_tokenize)\n",
    "dfl2['way6_text'] = df2['Text'].apply(preprocess_text)\n",
    "dfl2.to_pickle('way6_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal case + remove stopwords & punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# way7\n",
    "def remove_punc(text):\n",
    "    text = word_tokenize(text)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    punchless_bunch = [word.translate(translator) for word in text if word.translate(translator)]\n",
    "    return punchless_bunch\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokenized_sentences = sent_tokenize(text)\n",
    "    new_text = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        tokenized_words = remove_punc(sentence)\n",
    "        new_sentence = []\n",
    "        for word in tokenized_words:\n",
    "            if word not in stop_words:\n",
    "                new_sentence.append(word)\n",
    "        new_text.append(new_sentence)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = df.copy()\n",
    "dfb['tokenized_sentence'] = df['Text'].apply(sent_tokenize)\n",
    "dfb['way7_text'] = df['Text'].apply(preprocess_text)\n",
    "dfb.to_pickle('way7_train.pkl')\n",
    "\n",
    "dfb2 = df2.copy()\n",
    "dfb2['tokenized_sentence'] = df2['Text'].apply(sent_tokenize)\n",
    "dfb2['way7_text'] = df2['Text'].apply(preprocess_text)\n",
    "dfb2.to_pickle('way7_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase + remove stopwords & punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# way8\n",
    "def remove_punc(text):\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    punchless_bunch = [word.translate(translator) for word in text if word.translate(translator)]\n",
    "    return punchless_bunch\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokenized_sentences = sent_tokenize(text)\n",
    "    new_text = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        tokenized_words = remove_punc(sentence)\n",
    "        new_sentence = []\n",
    "        for word in tokenized_words:\n",
    "            if word not in stop_words:\n",
    "                new_sentence.append(word)\n",
    "        new_text.append(new_sentence)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df.copy()\n",
    "dfc['tokenized_sentence'] = df['Text'].apply(sent_tokenize)\n",
    "dfc['way8_text'] = df['Text'].apply(preprocess_text)\n",
    "dfc.to_pickle('way8_train.pkl')\n",
    "\n",
    "dfc2 = df2.copy()\n",
    "dfc2['tokenized_sentence'] = df2['Text'].apply(sent_tokenize)\n",
    "dfc2['way8_text'] = df2['Text'].apply(preprocess_text)\n",
    "dfc2.to_pickle('way8_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal case + remove stopwords & lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# way9\n",
    "def preprocess_text(text):   \n",
    "    tokenized_sentences = sent_tokenize(text)\n",
    "    new_text = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        tokenized_words = word_tokenize(sentence)\n",
    "        new_sentence = []\n",
    "        for word in tokenized_words:\n",
    "            if word not in stop_words:\n",
    "                lemmatized_word = lemmatizer.lemmatize(word)\n",
    "                new_sentence.append(lemmatized_word)\n",
    "        new_text.append(new_sentence)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = df.copy()\n",
    "dfd['tokenized_sentence'] = df['Text'].apply(sent_tokenize)\n",
    "dfd['way9_text'] = df['Text'].apply(preprocess_text)\n",
    "dfd.to_pickle('way9_train.pkl')\n",
    "\n",
    "dfd2 = df2.copy()\n",
    "dfd2['tokenized_sentence'] = df2['Text'].apply(sent_tokenize)\n",
    "dfd2['way9_text'] = df2['Text'].apply(preprocess_text)\n",
    "dfd2.to_pickle('way9_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal case + lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "IB5o1GXrpc8P"
   },
   "outputs": [],
   "source": [
    "# way10\n",
    "def preprocess_text(text):   \n",
    "    tokenized_sentences = sent_tokenize(text)\n",
    "    new_text = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        tokenized_words = word_tokenize(sentence)\n",
    "        new_sentence = []\n",
    "        for word in tokenized_words:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)\n",
    "            new_sentence.append(lemmatized_word)\n",
    "        new_text.append(new_sentence)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe = df.copy()\n",
    "dfe['tokenized_sentence'] = df['Text'].apply(sent_tokenize)\n",
    "dfe['way10_text'] = df['Text'].apply(preprocess_text)\n",
    "dfe.to_pickle('way10_train.pkl')\n",
    "\n",
    "dfe2 = df2.copy()\n",
    "dfe2['tokenized_sentence'] = df2['Text'].apply(sent_tokenize)\n",
    "dfe2['way10_text'] = df2['Text'].apply(preprocess_text)\n",
    "dfe2.to_pickle('way10_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal case + separate contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "LByc0cOZr6AT"
   },
   "outputs": [],
   "source": [
    "# way11\n",
    "\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"here's\": \"here is\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions_sentence(sentence, contractions):\n",
    "    for contraction, expansion in contractions.items():\n",
    "        pattern = re.compile(re.escape(contraction), re.IGNORECASE)\n",
    "        sentence = pattern.sub(expansion, sentence)\n",
    "    return sentence\n",
    "\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = [expand_contractions_sentence(sentence, contractions) for sentence in sentences]\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.copy()\n",
    "dff['tokenized_sentence'] = df['Text'].apply(sent_tokenize)\n",
    "dff['way11_text'] = df['Text'].apply(preprocess_text)\n",
    "dff.to_pickle('way11_train.pkl')\n",
    "\n",
    "dff2 = df2.copy()\n",
    "dff2['tokenized_sentence'] = df2['Text'].apply(sent_tokenize)\n",
    "dff2['way11_text'] = df2['Text'].apply(preprocess_text)\n",
    "dff2.to_pickle('way11_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "i--6nba6pJ3Q"
   },
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "IyRlR0OmpMnJ"
   },
   "outputs": [],
   "source": [
    "#tf-idf + Glove\n",
    "#Glove path\n",
    "glove_path = \"/content/drive/MyDrive/dsml/cs4248/as2/glove.6B.300d.txt\"\n",
    "\n",
    "def load_glove_dict(path):\n",
    "    embeddings_dict = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_dict = load_glove_dict(glove_path)\n",
    "\n",
    "def combine_glove_tfidf(texts, tfidf, tfidf_vectorizer, glove_dict):\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    vectors = np.zeros((len(texts), 300))\n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = text.split()\n",
    "        token_vectors = np.zeros((len(tokens), 300))\n",
    "        for j, token in enumerate(tokens):\n",
    "            if token in glove_dict:\n",
    "                glove_vector = glove_dict[token]\n",
    "                tfidf_index = tfidf_vectorizer.vocabulary_.get(token, -1)\n",
    "                if tfidf_index != -1:\n",
    "                    tfidf_value = tfidf[i, tfidf_index]\n",
    "                    token_vectors[j] = glove_vector * tfidf_value\n",
    "        if token_vectors.any():\n",
    "            vectors[i] = np.mean(token_vectors, axis=0)\n",
    "    return vectors\n",
    "\n",
    "X_train_combined = combine_glove_tfidf(X_train, X_train_tfidf, tfidf_vectorizer, glove_dict)\n",
    "X_test_combined = combine_glove_tfidf(X_test, X_test_tfidf, tfidf_vectorizer, glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkqxZvde0nSR"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_train_categorical = to_categorical(y_train_encoded)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "y_test_categorical = to_categorical(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
